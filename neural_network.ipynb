{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "3f2e6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "0538e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return 0 if x <= 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "7bf41176",
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = {\n",
    "    \"tanh\": 5/3,\n",
    "    \"relu\": 2**0.5,\n",
    "    \"sigmoid\": 1\n",
    "}\n",
    "\n",
    "activation_map = {\n",
    "    \"relu\": relu,\n",
    "    \"sigmoid\": sigmoid\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "03a9035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, arch, seed=99):\n",
    "        np.random.seed(99)\n",
    "        self.num_layers = len(arch)\n",
    "        self.params = {}\n",
    "        \n",
    "        for idx, layer in enumerate(arch):\n",
    "            layer_idx = idx + 1\n",
    "            \n",
    "            layer_input_size = layer[\"input_dim\"]\n",
    "            layer_output_size = layer[\"output_dim\"]\n",
    "            activation = layer[\"activation\"]\n",
    "            # kaiming initialization\n",
    "            kaiming_weight_init = gain[activation] / np.sqrt(layer_input_size)\n",
    "            self.params[\"w\" + str(layer_idx)] = np.full((layer_output_size, layer_input_size), kaiming_weight_init)\n",
    "            # self.params[\"w\" + str(layer_idx)] = np.random.randn(layer_output_size, layer_input_size) * 0.01\n",
    "            self.params[\"b\" + str(layer_idx)] = np.ones((layer_output_size, 1))\n",
    "            self.params[\"z\" + str(layer_idx)] = np.ones((layer_output_size, 1))\n",
    "            self.params[\"a\" + str(layer_idx)] = np.ones((layer_output_size, 1))\n",
    "            print(\"w\" + str(layer_idx), self.params[\"w\" + str(layer_idx)].shape)\n",
    "            print(\"b\" + str(layer_idx), self.params[\"b\" + str(layer_idx)].shape)\n",
    "            print(\"z\" + str(layer_idx), self.params[\"z\" + str(layer_idx)].shape)\n",
    "            print(\"a\" + str(layer_idx), self.params[\"a\" + str(layer_idx)].shape)\n",
    "        \n",
    "        self.params[\"loss\"] = 1\n",
    "        self.arch = arch\n",
    "        self.derivatives = {}\n",
    "        self.lr = 0.01\n",
    "        \n",
    "        \n",
    "    def forward_propogation(self, X):\n",
    "        self.params[\"a0\"] = X\n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            # z = wa + b\n",
    "            self.params[\"z\" + str(l)] = np.add(np.dot(self.params[\"w\" + str(l)], self.params[\"a\" + str(l-1)]), self.params[\"b\" + str(l)])\n",
    "            self.params[\"a\" + str(l)] = activation_map[self.arch[l-1][\"activation\"]](self.params[\"z\" + str(l)])\n",
    "            \n",
    "    def predict(self, x):\n",
    "        self.forward_propogation(x)\n",
    "        return self.params[\"a\" + str(self.num_layers)]\n",
    "            \n",
    "    def compute_loss(self, y):\n",
    "        self.params[\"loss\"] = -(y * np.log(self.params[\"a\" + str(self.num_layers)]) + (1-y) * np.log(1 - self.params[\"a\" + str(self.num_layers)]))\n",
    "        \n",
    "    def compute_derivatives(self, y):\n",
    "        # Partial derivatives of cost function wrt. z[l], w[l], b[l]\n",
    "        # dL/dz[l] = dL/da[l] * da[l]/dz[l]\n",
    "#         print(\"y: \", y)\n",
    "#         print('a' + str(self.num_layers), self.params['a' + str(self.num_layers)].shape)\n",
    "        self.derivatives['dz' + str(self.num_layers)] = self.params['a' + str(self.num_layers)] - y\n",
    "#         print('dz' + str(self.num_layers))\n",
    "#         print(self.derivatives['dz' + str(self.num_layers)].shape)\n",
    "        #dL/dw[l] = dL/dz[l] * dz[l]/dw[l]\n",
    "        #dz[l]/dw[l] = a[l-1]\n",
    "        self.derivatives['dw' + str(self.num_layers)] = np.dot(self.derivatives['dz' + str(self.num_layers)], np.transpose(self.params['a' + str(self.num_layers-1)]))\n",
    "#         print('dw' + str(self.num_layers))\n",
    "#         print(self.derivatives['dw' + str(self.num_layers)].shape)\n",
    "        # dL/db[l] = dL/dz[l] * dz[l]/db[l] = dL/dz[l]\n",
    "        self.derivatives['db' + str(self.num_layers)] = self.derivatives['dz' + str(self.num_layers)]\n",
    "#         print('db' + str(self.num_layers))\n",
    "#         print(self.derivatives['db' + str(self.num_layers)].shape)\n",
    "        \n",
    "        for l in range(self.num_layers-1, 0, -1):\n",
    "            # dz[l+1]/da[l] = w[l+1]\n",
    "            # da[l]/dz[l] = sigmoid_derivative(z[l])\n",
    "            # dL/dz[l] = dL/dz[l+1] * dz[l+1]/da[l] * da[l]/dz[l]\n",
    "#             print('dz' + str(l))\n",
    "#             print('w' + str(l+1), ' . ', 'dz' + str(l+1), ' * ', 'z' + str(l))\n",
    "            \n",
    "#             print(self.params['w' + str(l+1)].shape, self.derivatives['dz' + str(l+1)].shape, \n",
    "#                   sigmoid_derivative(self.params['z' + str(l)]).shape)\n",
    "            \n",
    "#             print(np.transpose(self.params['w' + str(l+1)]).shape, self.derivatives['dz' + str(l+1)].shape, \n",
    "#                   sigmoid_derivative(self.params['z' + str(l)]).shape)\n",
    "            \n",
    "            # broadcase operation *\n",
    "    \n",
    "            self.derivatives['dz' + str(l)] = np.dot(np.transpose(self.params['w' + str(l+1)]), \n",
    "                                                     self.derivatives['dz' + str(l+1)]) * \\\n",
    "                                                     sigmoid_derivative(self.params['z' + str(l)])\n",
    "            \n",
    "            # dL/dw[l] = dL/dz[l] * dz[l]/dw[l] = dL/dz[l] * a[l-1]\n",
    "            self.derivatives['dw' + str(l)] = np.dot(self.derivatives['dz' + str(l)], np.transpose(self.params['a' + str(l-1)]))\n",
    "            \n",
    "            # dL/db[l] = dL/z[l] * dz[l]/db = dL/z[l]\n",
    "            self.derivatives['db' + str(l)] = self.derivatives['dz' + str(l)]\n",
    "            \n",
    "        \n",
    "    def update_network_parameters(self):\n",
    "        for l in range(1, self.num_layers+1):\n",
    "            self.params[\"w\" + str(l)] -= self.lr * self.derivatives['dw' + str(l)]\n",
    "            self.params[\"b\" + str(l)] -= self.lr * self.derivatives['db' + str(l)]\n",
    "    \n",
    "    def backward_propogation(self, y):\n",
    "        \n",
    "        self.compute_derivatives(y)\n",
    "        self.update_network_parameters()\n",
    "        \n",
    "    def fit(self, X, Y, num_iter):\n",
    "        for iter in range(num_iter):\n",
    "            acc = 0\n",
    "            loss = 0\n",
    "            correct_predictions = 0\n",
    "            for i in range(X.shape[0]):\n",
    "#                print(\"x before: \", X[i])\n",
    "                x = X[i].reshape((X[i].size, 1))\n",
    "#                print(\"x after: \", X[i])\n",
    "                y = Y[i]\n",
    "                self.forward_propogation(x)\n",
    "                self.compute_loss(y)\n",
    "                loss += self.params[\"loss\"]\n",
    "                y_pred = self.predict(x)\n",
    "                y_pred = (y_pred > 0.5)\n",
    "                if y_pred.all() == y:\n",
    "                    correct_predictions += 1\n",
    "                self.backward_propogation(y)\n",
    "                \n",
    "            print('Iteration: ', iter)\n",
    "            print('Loss: ', loss)\n",
    "            print('Accuracy: ', (correct_predictions / X.shape[0]) * 100)\n",
    "            \n",
    "            \n",
    "    def matrix_fit(self, X, Y, num_iter):\n",
    "        acc = 0\n",
    "        loss = 0\n",
    "        correct_predictions = 0\n",
    "        for iter in range(num_iter):\n",
    "            # x = X[i].reshape((X[i].size, 1))\n",
    "            x = X[i].reshape((X[i].size, 1))\n",
    "            y = Y[i]\n",
    "            self.forward_propogation(x)\n",
    "            self.compute_loss(y)\n",
    "            loss = self.params[\"loss\"]\n",
    "            y_pred = self.predict(x)\n",
    "            y_pred = (y_pred > 0.5)\n",
    "            if y_pred.all() == y:\n",
    "                correct_predictions += 1\n",
    "            self.backward_propogation(y)\n",
    "            \n",
    "            print('Iteration: ', iter)\n",
    "            print('Loss: ', loss)\n",
    "        \n",
    "        print('Accuracy: ', (correct_predictions / num_iter) * 100)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "74e621d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('wheat-seeds-binary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "59e6a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "shuffled_dataset['Class'] = shuffled_dataset['Class'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "a4a6f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = shuffled_dataset.iloc[:, 0:-1].values\n",
    "y = shuffled_dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "fd6012be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X = StandardScaler()\n",
    "X = sc_X.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "27b3ce30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98, 7) (42, 7) (98,) (42,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "c0c5ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARCH = [\n",
    "#     {\"input_dim\": 7, \"output_dim\": 25, \"activation\": \"sigmoid\"},\n",
    "#     {\"input_dim\": 25, \"output_dim\": 50, \"activation\": \"sigmoid\"},\n",
    "# #    {\"input_dim\": 50, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "#     {\"input_dim\": 50, \"output_dim\": 25, \"activation\": \"sigmoid\"},\n",
    "#     {\"input_dim\": 25, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "62b87d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCH = [\n",
    "    {\"input_dim\": 7, \"output_dim\": 2, \"activation\": \"sigmoid\"},\n",
    "    {\"input_dim\": 2, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "c11b2b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 (2, 7)\n",
      "b1 (2, 1)\n",
      "z1 (2, 1)\n",
      "a1 (2, 1)\n",
      "w2 (1, 2)\n",
      "b2 (1, 1)\n",
      "z2 (1, 1)\n",
      "a2 (1, 1)\n",
      "Iteration:  0\n",
      "Loss:  [[1.5593111]]\n",
      "Iteration:  1\n",
      "Loss:  [[1.55090006]]\n",
      "Iteration:  2\n",
      "Loss:  [[1.5425552]]\n",
      "Iteration:  3\n",
      "Loss:  [[1.53427566]]\n",
      "Iteration:  4\n",
      "Loss:  [[1.52606063]]\n",
      "Iteration:  5\n",
      "Loss:  [[1.5179093]]\n",
      "Iteration:  6\n",
      "Loss:  [[1.50982088]]\n",
      "Iteration:  7\n",
      "Loss:  [[1.5017946]]\n",
      "Iteration:  8\n",
      "Loss:  [[1.49382969]]\n",
      "Iteration:  9\n",
      "Loss:  [[1.48592541]]\n",
      "Iteration:  10\n",
      "Loss:  [[1.47808103]]\n",
      "Iteration:  11\n",
      "Loss:  [[1.47029585]]\n",
      "Iteration:  12\n",
      "Loss:  [[1.46256915]]\n",
      "Iteration:  13\n",
      "Loss:  [[1.45490025]]\n",
      "Iteration:  14\n",
      "Loss:  [[1.44728849]]\n",
      "Iteration:  15\n",
      "Loss:  [[1.4397332]]\n",
      "Iteration:  16\n",
      "Loss:  [[1.43223374]]\n",
      "Iteration:  17\n",
      "Loss:  [[1.42478947]]\n",
      "Iteration:  18\n",
      "Loss:  [[1.41739978]]\n",
      "Iteration:  19\n",
      "Loss:  [[1.41006405]]\n",
      "Iteration:  20\n",
      "Loss:  [[1.40278169]]\n",
      "Iteration:  21\n",
      "Loss:  [[1.3955521]]\n",
      "Iteration:  22\n",
      "Loss:  [[1.38837473]]\n",
      "Iteration:  23\n",
      "Loss:  [[1.381249]]\n",
      "Iteration:  24\n",
      "Loss:  [[1.37417435]]\n",
      "Iteration:  25\n",
      "Loss:  [[1.36715026]]\n",
      "Iteration:  26\n",
      "Loss:  [[1.36017617]]\n",
      "Iteration:  27\n",
      "Loss:  [[1.35325158]]\n",
      "Iteration:  28\n",
      "Loss:  [[1.34637597]]\n",
      "Iteration:  29\n",
      "Loss:  [[1.33954882]]\n",
      "Iteration:  30\n",
      "Loss:  [[1.33276966]]\n",
      "Iteration:  31\n",
      "Loss:  [[1.32603798]]\n",
      "Iteration:  32\n",
      "Loss:  [[1.31935332]]\n",
      "Iteration:  33\n",
      "Loss:  [[1.3127152]]\n",
      "Iteration:  34\n",
      "Loss:  [[1.30612317]]\n",
      "Iteration:  35\n",
      "Loss:  [[1.29957676]]\n",
      "Iteration:  36\n",
      "Loss:  [[1.29307554]]\n",
      "Iteration:  37\n",
      "Loss:  [[1.28661906]]\n",
      "Iteration:  38\n",
      "Loss:  [[1.2802069]]\n",
      "Iteration:  39\n",
      "Loss:  [[1.27383863]]\n",
      "Iteration:  40\n",
      "Loss:  [[1.26751383]]\n",
      "Iteration:  41\n",
      "Loss:  [[1.2612321]]\n",
      "Iteration:  42\n",
      "Loss:  [[1.25499304]]\n",
      "Iteration:  43\n",
      "Loss:  [[1.24879624]]\n",
      "Iteration:  44\n",
      "Loss:  [[1.24264132]]\n",
      "Iteration:  45\n",
      "Loss:  [[1.23652789]]\n",
      "Iteration:  46\n",
      "Loss:  [[1.23045557]]\n",
      "Iteration:  47\n",
      "Loss:  [[1.224424]]\n",
      "Iteration:  48\n",
      "Loss:  [[1.21843281]]\n",
      "Iteration:  49\n",
      "Loss:  [[1.21248163]]\n",
      "Iteration:  50\n",
      "Loss:  [[1.20657012]]\n",
      "Iteration:  51\n",
      "Loss:  [[1.20069792]]\n",
      "Iteration:  52\n",
      "Loss:  [[1.19486469]]\n",
      "Iteration:  53\n",
      "Loss:  [[1.18907009]]\n",
      "Iteration:  54\n",
      "Loss:  [[1.18331378]]\n",
      "Iteration:  55\n",
      "Loss:  [[1.17759543]]\n",
      "Iteration:  56\n",
      "Loss:  [[1.17191473]]\n",
      "Iteration:  57\n",
      "Loss:  [[1.16627134]]\n",
      "Iteration:  58\n",
      "Loss:  [[1.16066496]]\n",
      "Iteration:  59\n",
      "Loss:  [[1.15509527]]\n",
      "Iteration:  60\n",
      "Loss:  [[1.14956197]]\n",
      "Iteration:  61\n",
      "Loss:  [[1.14406475]]\n",
      "Iteration:  62\n",
      "Loss:  [[1.13860332]]\n",
      "Iteration:  63\n",
      "Loss:  [[1.13317737]]\n",
      "Iteration:  64\n",
      "Loss:  [[1.12778662]]\n",
      "Iteration:  65\n",
      "Loss:  [[1.12243079]]\n",
      "Iteration:  66\n",
      "Loss:  [[1.11710958]]\n",
      "Iteration:  67\n",
      "Loss:  [[1.11182272]]\n",
      "Iteration:  68\n",
      "Loss:  [[1.10656993]]\n",
      "Iteration:  69\n",
      "Loss:  [[1.10135094]]\n",
      "Iteration:  70\n",
      "Loss:  [[1.09616548]]\n",
      "Iteration:  71\n",
      "Loss:  [[1.09101329]]\n",
      "Iteration:  72\n",
      "Loss:  [[1.08589409]]\n",
      "Iteration:  73\n",
      "Loss:  [[1.08080765]]\n",
      "Iteration:  74\n",
      "Loss:  [[1.07575368]]\n",
      "Iteration:  75\n",
      "Loss:  [[1.07073195]]\n",
      "Iteration:  76\n",
      "Loss:  [[1.0657422]]\n",
      "Iteration:  77\n",
      "Loss:  [[1.06078419]]\n",
      "Iteration:  78\n",
      "Loss:  [[1.05585767]]\n",
      "Iteration:  79\n",
      "Loss:  [[1.05096239]]\n",
      "Iteration:  80\n",
      "Loss:  [[1.04609813]]\n",
      "Iteration:  81\n",
      "Loss:  [[1.04126464]]\n",
      "Iteration:  82\n",
      "Loss:  [[1.03646168]]\n",
      "Iteration:  83\n",
      "Loss:  [[1.03168904]]\n",
      "Iteration:  84\n",
      "Loss:  [[1.02694648]]\n",
      "Iteration:  85\n",
      "Loss:  [[1.02223377]]\n",
      "Iteration:  86\n",
      "Loss:  [[1.01755069]]\n",
      "Iteration:  87\n",
      "Loss:  [[1.01289702]]\n",
      "Iteration:  88\n",
      "Loss:  [[1.00827254]]\n",
      "Iteration:  89\n",
      "Loss:  [[1.00367703]]\n",
      "Iteration:  90\n",
      "Loss:  [[0.99911028]]\n",
      "Iteration:  91\n",
      "Loss:  [[0.99457208]]\n",
      "Iteration:  92\n",
      "Loss:  [[0.99006222]]\n",
      "Iteration:  93\n",
      "Loss:  [[0.98558048]]\n",
      "Iteration:  94\n",
      "Loss:  [[0.98112666]]\n",
      "Iteration:  95\n",
      "Loss:  [[0.97670056]]\n",
      "Iteration:  96\n",
      "Loss:  [[0.97230198]]\n",
      "Iteration:  97\n",
      "Loss:  [[0.96793071]]\n",
      "Iteration:  98\n",
      "Loss:  [[0.96358657]]\n",
      "Iteration:  99\n",
      "Loss:  [[0.95926934]]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "classifier = NeuralNetwork(ARCH)\n",
    "# classifier.fit(X_train, y_train, 1)\n",
    "classifier.matrix_fit(X_train, y_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "ec725bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 45.23809523809524\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "n_c = 0\n",
    "for i in range(0, X_test.shape[0]):\n",
    "  x = X_test[i].reshape((X_test[i].size, 1))\n",
    "  y = y_test[i]\n",
    "  y_pred = classifier.predict(x)\n",
    "  y_pred = (y_pred > 0.5)\n",
    "  #print('Expected: %d Got: %d' %(y, y_pred))\n",
    "  if y_pred == y:\n",
    "      n_c += 1\n",
    "\n",
    "acc = (n_c/X_test.shape[0])*100\n",
    "print(\"Test Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "25fd030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "#import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.utils import np_utils\n",
    "# from keras import regularizers\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "45a16b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# number of samples in the data set\n",
    "N_SAMPLES = 1000\n",
    "# ratio between training and test sets\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "X, y = make_moons(n_samples = N_SAMPLES, noise=0.2, random_state=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "45d9c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCH = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 25, \"activation\": \"sigmoid\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 50, \"activation\": \"sigmoid\"},\n",
    "    {\"input_dim\": 50, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 50, \"output_dim\": 25, \"activation\": \"sigmoid\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "3d074988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 (25, 2)\n",
      "b1 (25, 1)\n",
      "z1 (25, 1)\n",
      "a1 (25, 1)\n",
      "w2 (50, 25)\n",
      "b2 (50, 1)\n",
      "z2 (50, 1)\n",
      "a2 (50, 1)\n",
      "w3 (50, 50)\n",
      "b3 (50, 1)\n",
      "z3 (50, 1)\n",
      "a3 (50, 1)\n",
      "w4 (25, 50)\n",
      "b4 (25, 1)\n",
      "z4 (25, 1)\n",
      "a4 (25, 1)\n",
      "w5 (1, 25)\n",
      "b5 (1, 1)\n",
      "z5 (1, 1)\n",
      "a5 (1, 1)\n",
      "Iteration:  0\n",
      "Loss:  [[6.00247569]]\n",
      "Iteration:  1\n",
      "Loss:  [[5.74385043]]\n",
      "Iteration:  2\n",
      "Loss:  [[5.48563005]]\n",
      "Iteration:  3\n",
      "Loss:  [[5.2279325]]\n",
      "Iteration:  4\n",
      "Loss:  [[4.97090926]]\n",
      "Iteration:  5\n",
      "Loss:  [[4.71475429]]\n",
      "Iteration:  6\n",
      "Loss:  [[4.45971502]]\n",
      "Iteration:  7\n",
      "Loss:  [[4.20610552]]\n",
      "Iteration:  8\n",
      "Loss:  [[3.95432197]]\n",
      "Iteration:  9\n",
      "Loss:  [[3.70486017]]\n",
      "Iteration:  10\n",
      "Loss:  [[3.45833412]]\n",
      "Iteration:  11\n",
      "Loss:  [[3.21549424]]\n",
      "Iteration:  12\n",
      "Loss:  [[2.97724231]]\n",
      "Iteration:  13\n",
      "Loss:  [[2.74463885]]\n",
      "Iteration:  14\n",
      "Loss:  [[2.51889737]]\n",
      "Iteration:  15\n",
      "Loss:  [[2.30135895]]\n",
      "Iteration:  16\n",
      "Loss:  [[2.09344133]]\n",
      "Iteration:  17\n",
      "Loss:  [[1.89655997]]\n",
      "Iteration:  18\n",
      "Loss:  [[1.71202483]]\n",
      "Iteration:  19\n",
      "Loss:  [[1.54092516]]\n",
      "Iteration:  20\n",
      "Loss:  [[1.38402217]]\n",
      "Iteration:  21\n",
      "Loss:  [[1.2416725]]\n",
      "Iteration:  22\n",
      "Loss:  [[1.11380031]]\n",
      "Iteration:  23\n",
      "Loss:  [[0.99992354]]\n",
      "Iteration:  24\n",
      "Loss:  [[0.89922596]]\n",
      "Iteration:  25\n",
      "Loss:  [[0.81065595]]\n",
      "Iteration:  26\n",
      "Loss:  [[0.73303065]]\n",
      "Iteration:  27\n",
      "Loss:  [[0.66512819]]\n",
      "Iteration:  28\n",
      "Loss:  [[0.60575852]]\n",
      "Iteration:  29\n",
      "Loss:  [[0.5538107]]\n",
      "Iteration:  30\n",
      "Loss:  [[0.50827908]]\n",
      "Iteration:  31\n",
      "Loss:  [[0.46827353]]\n",
      "Iteration:  32\n",
      "Loss:  [[0.43301869]]\n",
      "Iteration:  33\n",
      "Loss:  [[0.4018467]]\n",
      "Iteration:  34\n",
      "Loss:  [[0.37418657]]\n",
      "Iteration:  35\n",
      "Loss:  [[0.3495524]]\n",
      "Iteration:  36\n",
      "Loss:  [[0.3275317]]\n",
      "Iteration:  37\n",
      "Loss:  [[0.30777458]]\n",
      "Iteration:  38\n",
      "Loss:  [[0.28998417]]\n",
      "Iteration:  39\n",
      "Loss:  [[0.27390823]]\n",
      "Iteration:  40\n",
      "Loss:  [[0.25933202]]\n",
      "Iteration:  41\n",
      "Loss:  [[0.24607229]]\n",
      "Iteration:  42\n",
      "Loss:  [[0.23397219]]\n",
      "Iteration:  43\n",
      "Loss:  [[0.22289711]]\n",
      "Iteration:  44\n",
      "Loss:  [[0.2127311]]\n",
      "Iteration:  45\n",
      "Loss:  [[0.203374]]\n",
      "Iteration:  46\n",
      "Loss:  [[0.19473902]]\n",
      "Iteration:  47\n",
      "Loss:  [[0.18675064]]\n",
      "Iteration:  48\n",
      "Loss:  [[0.17934299]]\n",
      "Iteration:  49\n",
      "Loss:  [[0.17245844]]\n",
      "Iteration:  50\n",
      "Loss:  [[0.16604635]]\n",
      "Iteration:  51\n",
      "Loss:  [[0.16006215]]\n",
      "Iteration:  52\n",
      "Loss:  [[0.15446645]]\n",
      "Iteration:  53\n",
      "Loss:  [[0.14922437]]\n",
      "Iteration:  54\n",
      "Loss:  [[0.14430493]]\n",
      "Iteration:  55\n",
      "Loss:  [[0.13968051]]\n",
      "Iteration:  56\n",
      "Loss:  [[0.13532645]]\n",
      "Iteration:  57\n",
      "Loss:  [[0.13122068]]\n",
      "Iteration:  58\n",
      "Loss:  [[0.12734337]]\n",
      "Iteration:  59\n",
      "Loss:  [[0.12367671]]\n",
      "Iteration:  60\n",
      "Loss:  [[0.1202046]]\n",
      "Iteration:  61\n",
      "Loss:  [[0.11691253]]\n",
      "Iteration:  62\n",
      "Loss:  [[0.11378731]]\n",
      "Iteration:  63\n",
      "Loss:  [[0.11081702]]\n",
      "Iteration:  64\n",
      "Loss:  [[0.10799079]]\n",
      "Iteration:  65\n",
      "Loss:  [[0.10529871]]\n",
      "Iteration:  66\n",
      "Loss:  [[0.10273177]]\n",
      "Iteration:  67\n",
      "Loss:  [[0.10028169]]\n",
      "Iteration:  68\n",
      "Loss:  [[0.0979409]]\n",
      "Iteration:  69\n",
      "Loss:  [[0.09570248]]\n",
      "Iteration:  70\n",
      "Loss:  [[0.09356003]]\n",
      "Iteration:  71\n",
      "Loss:  [[0.09150768]]\n",
      "Iteration:  72\n",
      "Loss:  [[0.08954002]]\n",
      "Iteration:  73\n",
      "Loss:  [[0.08765205]]\n",
      "Iteration:  74\n",
      "Loss:  [[0.08583916]]\n",
      "Iteration:  75\n",
      "Loss:  [[0.08409706]]\n",
      "Iteration:  76\n",
      "Loss:  [[0.08242178]]\n",
      "Iteration:  77\n",
      "Loss:  [[0.08080966]]\n",
      "Iteration:  78\n",
      "Loss:  [[0.07925726]]\n",
      "Iteration:  79\n",
      "Loss:  [[0.07776141]]\n",
      "Iteration:  80\n",
      "Loss:  [[0.07631915]]\n",
      "Iteration:  81\n",
      "Loss:  [[0.0749277]]\n",
      "Iteration:  82\n",
      "Loss:  [[0.07358449]]\n",
      "Iteration:  83\n",
      "Loss:  [[0.07228711]]\n",
      "Iteration:  84\n",
      "Loss:  [[0.0710333]]\n",
      "Iteration:  85\n",
      "Loss:  [[0.06982095]]\n",
      "Iteration:  86\n",
      "Loss:  [[0.06864808]]\n",
      "Iteration:  87\n",
      "Loss:  [[0.06751283]]\n",
      "Iteration:  88\n",
      "Loss:  [[0.06641344]]\n",
      "Iteration:  89\n",
      "Loss:  [[0.06534829]]\n",
      "Iteration:  90\n",
      "Loss:  [[0.06431583]]\n",
      "Iteration:  91\n",
      "Loss:  [[0.06331461]]\n",
      "Iteration:  92\n",
      "Loss:  [[0.06234325]]\n",
      "Iteration:  93\n",
      "Loss:  [[0.06140046]]\n",
      "Iteration:  94\n",
      "Loss:  [[0.06048502]]\n",
      "Iteration:  95\n",
      "Loss:  [[0.05959579]]\n",
      "Iteration:  96\n",
      "Loss:  [[0.05873166]]\n",
      "Iteration:  97\n",
      "Loss:  [[0.05789162]]\n",
      "Iteration:  98\n",
      "Loss:  [[0.05707468]]\n",
      "Iteration:  99\n",
      "Loss:  [[0.05627992]]\n",
      "Accuracy:  73.0\n"
     ]
    }
   ],
   "source": [
    "classifier = NeuralNetwork(ARCH)\n",
    "# classifier.fit(X_train, y_train, 1)\n",
    "classifier.matrix_fit(X_train, y_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "1b3c5bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReferences:\\n\\nhttps://medium.com/binaryandmore/beginners-guide-to-deriving-and-implementing-backpropagation-e3c1a5a1e536#68b5\\nhttps://github.com/pranavbudhwant/backpropagation-in-numpy/blob/master/Implementation_Notebook.ipynb\\n\\nhttps://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795\\nhttps://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/03_numpy_neural_net/Numpy%20deep%20neural%20network.ipynb\\n\\n'"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "References:\n",
    "\n",
    "https://medium.com/binaryandmore/beginners-guide-to-deriving-and-implementing-backpropagation-e3c1a5a1e536#68b5\n",
    "https://github.com/pranavbudhwant/backpropagation-in-numpy/blob/master/Implementation_Notebook.ipynb\n",
    "\n",
    "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795\n",
    "https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/03_numpy_neural_net/Numpy%20deep%20neural%20network.ipynb\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c03591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2251df83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

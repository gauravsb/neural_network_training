{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "08a897b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "0086b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return 0 if x <= 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "50b015ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = {\n",
    "    \"tanh\": 5/3,\n",
    "    \"relu\": 2**0.5,\n",
    "    \"sigmoid\": 1\n",
    "}\n",
    "\n",
    "activation_map = {\n",
    "    \"relu\": relu,\n",
    "    \"sigmoid\": sigmoid\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "fcdef90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, arch, seed=99):\n",
    "        np.random.seed(99)\n",
    "        self.num_layers = len(arch)\n",
    "        self.params = {}\n",
    "        \n",
    "        for idx, layer in enumerate(arch):\n",
    "            layer_idx = idx + 1\n",
    "            \n",
    "            layer_input_size = layer[\"input_dim\"]\n",
    "            layer_output_size = layer[\"output_dim\"]\n",
    "            activation = layer[\"activation\"]\n",
    "            # kaiming initialization\n",
    "            kaiming_weight_init = gain[activation] / np.sqrt(layer_input_size)\n",
    "            self.params[\"w\" + str(layer_idx)] = np.full((layer_output_size, layer_input_size), kaiming_weight_init)\n",
    "            \n",
    "            # random weight initialization\n",
    "            # self.params[\"w\" + str(layer_idx)] = np.random.randn(layer_output_size, layer_input_size) * 0.01\n",
    "            self.params[\"b\" + str(layer_idx)] = np.ones((layer_output_size, 1))\n",
    "            self.params[\"z\" + str(layer_idx)] = np.ones((layer_output_size, 1))\n",
    "            self.params[\"a\" + str(layer_idx)] = np.ones((layer_output_size, 1))\n",
    "            print(\"w\" + str(layer_idx), self.params[\"w\" + str(layer_idx)].shape)\n",
    "            print(\"b\" + str(layer_idx), self.params[\"b\" + str(layer_idx)].shape)\n",
    "            print(\"z\" + str(layer_idx), self.params[\"z\" + str(layer_idx)].shape)\n",
    "            print(\"a\" + str(layer_idx), self.params[\"a\" + str(layer_idx)].shape)\n",
    "        \n",
    "        self.params[\"loss\"] = 1\n",
    "        self.arch = arch\n",
    "        self.derivatives = {}\n",
    "        self.lr = 0.01\n",
    "        \n",
    "        \n",
    "    def forward_propogation(self, X):\n",
    "        self.params[\"a0\"] = X\n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            # z = wa + b\n",
    "            self.params[\"z\" + str(l)] = np.add(np.dot(self.params[\"w\" + str(l)], self.params[\"a\" + str(l-1)]), self.params[\"b\" + str(l)])\n",
    "            self.params[\"a\" + str(l)] = activation_map[self.arch[l-1][\"activation\"]](self.params[\"z\" + str(l)])\n",
    "            \n",
    "    def predict(self, x):\n",
    "        self.forward_propogation(x)\n",
    "        return self.params[\"a\" + str(self.num_layers)]\n",
    "            \n",
    "    def compute_loss(self, y):\n",
    "        self.params[\"loss\"] = -(y * np.log(self.params[\"a\" + str(self.num_layers)]) + (1-y) * np.log(1 - self.params[\"a\" + str(self.num_layers)]))\n",
    "        \n",
    "    def compute_derivatives(self, y):\n",
    "        # Partial derivatives of cost function wrt. z[l], w[l], b[l]\n",
    "        # dL/dz[l] = dL/da[l] * da[l]/dz[l]\n",
    "#         print(\"y: \", y)\n",
    "#         print('a' + str(self.num_layers), self.params['a' + str(self.num_layers)].shape)\n",
    "        self.derivatives['dz' + str(self.num_layers)] = self.params['a' + str(self.num_layers)] - y\n",
    "#         print('dz' + str(self.num_layers))\n",
    "#         print(self.derivatives['dz' + str(self.num_layers)].shape)\n",
    "        #dL/dw[l] = dL/dz[l] * dz[l]/dw[l]\n",
    "        #dz[l]/dw[l] = a[l-1]\n",
    "        self.derivatives['dw' + str(self.num_layers)] = np.dot(self.derivatives['dz' + str(self.num_layers)], np.transpose(self.params['a' + str(self.num_layers-1)]))\n",
    "#         print('dw' + str(self.num_layers))\n",
    "#         print(self.derivatives['dw' + str(self.num_layers)].shape)\n",
    "        # dL/db[l] = dL/dz[l] * dz[l]/db[l] = dL/dz[l]\n",
    "        self.derivatives['db' + str(self.num_layers)] = self.derivatives['dz' + str(self.num_layers)]\n",
    "#         print('db' + str(self.num_layers))\n",
    "#         print(self.derivatives['db' + str(self.num_layers)].shape)\n",
    "        \n",
    "        for l in range(self.num_layers-1, 0, -1):\n",
    "            # dz[l+1]/da[l] = w[l+1]\n",
    "            # da[l]/dz[l] = sigmoid_derivative(z[l])\n",
    "            # dL/dz[l] = dL/dz[l+1] * dz[l+1]/da[l] * da[l]/dz[l]\n",
    "#             print('dz' + str(l))\n",
    "#             print('w' + str(l+1), ' . ', 'dz' + str(l+1), ' * ', 'z' + str(l))\n",
    "            \n",
    "#             print(self.params['w' + str(l+1)].shape, self.derivatives['dz' + str(l+1)].shape, \n",
    "#                   sigmoid_derivative(self.params['z' + str(l)]).shape)\n",
    "            \n",
    "#             print(np.transpose(self.params['w' + str(l+1)]).shape, self.derivatives['dz' + str(l+1)].shape, \n",
    "#                   sigmoid_derivative(self.params['z' + str(l)]).shape)\n",
    "            \n",
    "            # broadcase operation *\n",
    "    \n",
    "            self.derivatives['dz' + str(l)] = np.dot(np.transpose(self.params['w' + str(l+1)]), \n",
    "                                                     self.derivatives['dz' + str(l+1)]) * \\\n",
    "                                                     sigmoid_derivative(self.params['z' + str(l)])\n",
    "            \n",
    "            # dL/dw[l] = dL/dz[l] * dz[l]/dw[l] = dL/dz[l] * a[l-1]\n",
    "            self.derivatives['dw' + str(l)] = np.dot(self.derivatives['dz' + str(l)], np.transpose(self.params['a' + str(l-1)]))\n",
    "            \n",
    "            # dL/db[l] = dL/z[l] * dz[l]/db = dL/z[l]\n",
    "            self.derivatives['db' + str(l)] = self.derivatives['dz' + str(l)]\n",
    "            \n",
    "        \n",
    "    def update_network_parameters(self):\n",
    "        for l in range(1, self.num_layers+1):\n",
    "            self.params[\"w\" + str(l)] -= self.lr * self.derivatives['dw' + str(l)]\n",
    "            self.params[\"b\" + str(l)] -= self.lr * self.derivatives['db' + str(l)]\n",
    "    \n",
    "    def backward_propogation(self, y):\n",
    "        \n",
    "        self.compute_derivatives(y)\n",
    "        self.update_network_parameters()\n",
    "        \n",
    "    def fit(self, X, Y, num_iter):\n",
    "        for iter in range(num_iter):\n",
    "            acc = 0\n",
    "            loss = 0\n",
    "            correct_predictions = 0\n",
    "            for i in range(X.shape[0]):\n",
    "#                print(\"x before: \", X[i])\n",
    "                x = X[i].reshape((X[i].size, 1))\n",
    "#                print(\"x after: \", X[i])\n",
    "                y = Y[i]\n",
    "                self.forward_propogation(x)\n",
    "                self.compute_loss(y)\n",
    "                loss += self.params[\"loss\"]\n",
    "                y_pred = self.predict(x)\n",
    "                y_pred = (y_pred > 0.5)\n",
    "                if y_pred.all() == y:\n",
    "                    correct_predictions += 1\n",
    "                self.backward_propogation(y)\n",
    "                \n",
    "            print('Iteration: ', iter)\n",
    "            print('Loss: ', loss)\n",
    "            print('Accuracy: ', (correct_predictions / X.shape[0]) * 100)\n",
    "            \n",
    "            \n",
    "    def matrix_fit(self, X, Y, num_iter):\n",
    "        acc = 0\n",
    "        loss = 0\n",
    "        correct_predictions = 0\n",
    "        for iter in range(num_iter):\n",
    "            # x = X[i].reshape((X[i].size, 1))\n",
    "            x = X[i].reshape((X[i].size, 1))\n",
    "            y = Y[i]\n",
    "            self.forward_propogation(x)\n",
    "            self.compute_loss(y)\n",
    "            loss = self.params[\"loss\"]\n",
    "            y_pred = self.predict(x)\n",
    "            y_pred = (y_pred > 0.5)\n",
    "            if y_pred.all() == y:\n",
    "                correct_predictions += 1\n",
    "            self.backward_propogation(y)\n",
    "            \n",
    "            print('Iteration: ', iter)\n",
    "            print('Loss: ', loss)\n",
    "        \n",
    "        print('Accuracy: ', (correct_predictions / num_iter) * 100)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "22d3cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('wheat-seeds-binary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "9763cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "shuffled_dataset['Class'] = shuffled_dataset['Class'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "7684909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = shuffled_dataset.iloc[:, 0:-1].values\n",
    "y = shuffled_dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "0648b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X = StandardScaler()\n",
    "X = sc_X.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "3079dce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98, 7) (42, 7) (98,) (42,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "321294fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARCH = [\n",
    "#     {\"input_dim\": 7, \"output_dim\": 25, \"activation\": \"sigmoid\"},\n",
    "#     {\"input_dim\": 25, \"output_dim\": 50, \"activation\": \"sigmoid\"},\n",
    "# #    {\"input_dim\": 50, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "#     {\"input_dim\": 50, \"output_dim\": 25, \"activation\": \"sigmoid\"},\n",
    "#     {\"input_dim\": 25, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "3f736196",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCH = [\n",
    "    {\"input_dim\": 7, \"output_dim\": 2, \"activation\": \"sigmoid\"},\n",
    "    {\"input_dim\": 2, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "ae45af32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 (2, 7)\n",
      "b1 (2, 1)\n",
      "z1 (2, 1)\n",
      "a1 (2, 1)\n",
      "w2 (1, 2)\n",
      "b2 (1, 1)\n",
      "z2 (1, 1)\n",
      "a2 (1, 1)\n",
      "Iteration:  0\n",
      "Loss:  [[1.5593111]]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "classifier = NeuralNetwork(ARCH)\n",
    "# classifier.fit(X_train, y_train, 1)\n",
    "classifier.matrix_fit(X_train, y_train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "61d66bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 45.23809523809524\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "n_c = 0\n",
    "for i in range(0, X_test.shape[0]):\n",
    "  x = X_test[i].reshape((X_test[i].size, 1))\n",
    "  y = y_test[i]\n",
    "  y_pred = classifier.predict(x)\n",
    "  y_pred = (y_pred > 0.5)\n",
    "  #print('Expected: %d Got: %d' %(y, y_pred))\n",
    "  if y_pred == y:\n",
    "      n_c += 1\n",
    "\n",
    "acc = (n_c/X_test.shape[0])*100\n",
    "print(\"Test Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "25c05906",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "#import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.utils import np_utils\n",
    "# from keras import regularizers\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "a5176568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# number of samples in the data set\n",
    "N_SAMPLES = 1000\n",
    "# ratio between training and test sets\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "X, y = make_moons(n_samples = N_SAMPLES, noise=0.2, random_state=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "1acf09e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCH = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 25, \"activation\": \"sigmoid\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 50, \"activation\": \"sigmoid\"},\n",
    "    {\"input_dim\": 50, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 50, \"output_dim\": 25, \"activation\": \"sigmoid\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "a36baa28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 (25, 2)\n",
      "b1 (25, 1)\n",
      "z1 (25, 1)\n",
      "a1 (25, 1)\n",
      "w2 (50, 25)\n",
      "b2 (50, 1)\n",
      "z2 (50, 1)\n",
      "a2 (50, 1)\n",
      "w3 (50, 50)\n",
      "b3 (50, 1)\n",
      "z3 (50, 1)\n",
      "a3 (50, 1)\n",
      "w4 (25, 50)\n",
      "b4 (25, 1)\n",
      "z4 (25, 1)\n",
      "a4 (25, 1)\n",
      "w5 (1, 25)\n",
      "b5 (1, 1)\n",
      "z5 (1, 1)\n",
      "a5 (1, 1)\n",
      "Iteration:  0\n",
      "Loss:  [[6.00247569]]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "classifier = NeuralNetwork(ARCH)\n",
    "# classifier.fit(X_train, y_train, 1)\n",
    "classifier.matrix_fit(X_train, y_train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "eb558281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReferences:\\n\\nhttps://medium.com/binaryandmore/beginners-guide-to-deriving-and-implementing-backpropagation-e3c1a5a1e536#68b5\\nhttps://github.com/pranavbudhwant/backpropagation-in-numpy/blob/master/Implementation_Notebook.ipynb\\n\\nhttps://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795\\nhttps://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/03_numpy_neural_net/Numpy%20deep%20neural%20network.ipynb\\n\\n'"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "References:\n",
    "\n",
    "https://medium.com/binaryandmore/beginners-guide-to-deriving-and-implementing-backpropagation-e3c1a5a1e536#68b5\n",
    "https://github.com/pranavbudhwant/backpropagation-in-numpy/blob/master/Implementation_Notebook.ipynb\n",
    "\n",
    "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795\n",
    "https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/03_numpy_neural_net/Numpy%20deep%20neural%20network.ipynb\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f500f51f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
